{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 04\n",
    "\n",
    "## Inleiding\n",
    "\n",
    "Dit is de werkcollege-oefening bij Les 04 van het vak *Advanced Datamining* (BFVH4DMN2). Bestudeer eerst de syllabus behorende bij deze les. Op BlackBoard kun je naast dit iPython/Jupyter notebook een tweetal Python-bestanden vinden. Sla deze op in dezelfde folder als dit notebook. Het verdient aanbeveling om voor elke les een aparte folder aan te maken.\n",
    "\n",
    "- **template.py** bevat een opzet voor een module met object-georiënteerde implementaties van neurale netwerk algoritmen. Het doel van deze oefening is om deze code aan te vullen en uit te werken tot een correct werkend model. Open dit bestand in een code-editor naar keuze en sla het op als **model.py**. Vergeet niet om tijdens het uitwerken van deze oefening je aanpassingen in de editor telkens op te slaan voordat je de code in dit notebook uitvoert!\n",
    "\n",
    "- **data.py** bevat een aantal functies die helpen bij het genereren en het visualiseren van de gebruikte datasets. Deze functies hoeven maar één keer ingelezen te worden en hoef je niet te wijzigen om deze opdracht correct uit te kunnen voeren.\n",
    "\n",
    "Laten we dus beginnen om deze functies te importeren, samen met wat initialisatie-code: plaats de cursor in de cel hieronder en druk op Shift+Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from importlib import reload\n",
    "\n",
    "import model, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "We gaan in dit werkcollege *multinomiale* classificatie implementeren. Dat wil zeggen dat de dataset meer dan twee klasselabels zal bevatten. De nieuwe dataset zal worden gesampled uit een *fractal*. De details doen er hier niet zoveel toe, maar een kenmerk van fractals is dat ze op elke grootte-schaal details bevatten. Een neuraal netwerk kan onmogelijk al die kenmerken fitten, maar het is interessant om te kijken hoe ver een model kan komen. Grafisch ziet de fractal die wij zullen gebruiken er als volgt uit.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/db/Julia_set_for_the_rational_function.png\" width=\"320\" height=\"240\" style=\"-moz-transform: scaleX(-1); -o-transform: scaleX(-1); -webkit-transform: scaleX(-1); transform: scaleX(-1); filter: FlipH; -ms-filter: \"FlipH\";\">\n",
    "\n",
    "De data die wij zullen bekijken kent twee attributen en drie verschillende klassen. De drie klassen zijn hier weergegeven in de kleuren groen, rood en blauw. Opmerkelijk aan deze fractal is dat op de grens tussen twee klassen altijd \"eilandjes\" liggen van de derde klasse; maar op de grens met die eilandjes liggen ook weer kleinere eilandjes; enzovoorts. Je kunt je afvragen of de klassen wel echt paarsgewijs aan elkaar grenzen, aangezien er altijd de andere klasse nog weer tussen ligt, maar dat terzijde.\n",
    "\n",
    "Er bestaan ook versies met meer dan drie klassen. Gegeven is een functie `data.multinomial()` die een steekproef neemt van een bepaald aantal punten uit deze dataset. Een verplicht argument geeft aan hoeveel klassen we willen kiezen. Standaard worden er 256 datapunten gegenereerd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(data.multinomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laten we beginnen een dataset met drie klassen te genereren. Elke instance heeft twee attributen $x_1$ en $x_2$; het klasselabel wordt gegeven in de vorm van een one-hot encoding en bestaat daarmee uit een lijst met drie waarden $y_1$, $y_2$ en $y_3$ die gelijk zijn aan 0 of 1. Wanneer we de data plotten met de `data.scatter()` functie worden er drie plotjes gemaakt, voor elke klasse één. Ga voor jezelf na dat deze overeenkomen met respectievelijk de groene, rode en blauwe klassen uit de afbeelding hierboven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = data.multinomial(3)\n",
    "print('Eerste instance:')\n",
    "print('Attributen   x =', xs[0])\n",
    "print('Klasselabels y =', ys[0])\n",
    "data.scatter(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kunnen eens beginnen met deze data te fitten met een multi-layer perceptron zoals we dat in de vorige les hebben geïmplementeerd. Neem de diverse activatie- en loss-functies samen met de code voor de classes `Layer()`, `FullLayer()` en `LossLayer` over uit de vorige les. Als het goed is werkt het onderstaande model nu \"out of the box\".\n",
    "\n",
    "De learning rate $\\alpha$ is hier gelijk genomen aan `alpha=0.1`. De hidden layer bevat acht neuronen met een tanh-activatiefunctie. Dit zijn voor dit probleem redelijke waarden, maar voel je vrij deze zonodig aan te passen. De breedte van de input layer dient wel op twee gesteld te worden en die van de output layer op drie. (Waarom?)\n",
    "\n",
    "Convergeert het model ogenschijnlijk naar een enigszins redelijke oplossing? Als het goed is worden in elke grafiek de punten van één klasse geïdentificeerd (in blauw) ten opzichte van de andere (in geel). De voorspellingen van het model (de achtergrondkleur) dienen daar redelijkerwijs mee overeen te komen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model)\n",
    "\n",
    "hidden = model.FullLayer(2, 8, act_func=model.tanh_act_func)\n",
    "output = model.FullLayer(8, 3, act_func=model.identity_act_func)\n",
    "loss = model.LossLayer(loss_func=model.quadratic_loss_func)\n",
    "my_network = hidden + output + loss\n",
    "my_network.fit(xs, ys, alpha=0.1)\n",
    "data.scatter(xs, ys, model=my_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De uitkomsten van het model hierboven hebben een bereik van -1 (rood) tot +1 (blauw), of vallen daar zelfs nog buiten wanneer de kleuren worden geclipt. Hoewel de klasselabels gegeven zijn door middel van nullen en enen kan de uitvoer van het model dus *niet* gezien worden als een schatting van de kans op een zeker klasselabel. Immers, kansen kunnen nooit kleiner dan nul of groter dan één zijn. Dit is op te lossen met een *softmax* output layer. Het past niet zo goed in onze opzet om dit te implementeren in de vorm van een extra activatie-functie. Daarom zullen we een geheel nieuwe child-class `SoftmaxLayer()` van `Layer()` aanmaken.\n",
    "\n",
    "In het gegeven template worden de speciale `__init__()` en `__str__()` functies reeds gegeven; deze hoeven eigenlijk niet meer te worden aangepast. De softmax layer heeft geen parameters dus die hoeven ook niet te worden geïnitialiseerd of weergegeven. Wel zal de functie `predict()` moeten worden toegevoegd. Deze ontvangt een lijst met invoerwaarden `x` waarop de softmax-functie dient te worden toegepast om te komen tot de te retourneren uitvoerwaarden $\\hat{y}_n$ volgens\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y}_{n}=\\frac{e^{x_{n}}}{\\sum_{j}e^{x_{j}}}\n",
    "\\end{equation*}\n",
    "\n",
    "Bereken dus eerst een nieuwe lijst met $e^{x_{n}}$, bepaal hiervan de som, en deel dan alle waarden in die nieuwe lijst door de som.\n",
    "\n",
    "Test dat je functie werkt met de code hieronder. Controleer dat de uitkomst voldoet aan alle eisen die eraan gesteld mogen worden:\n",
    "\n",
    "- alle $\\hat{y}_n$ zijn groter of gelijk aan nul;\n",
    "\n",
    "- de som van alle $\\hat{y}_n$ is gelijk aan 1;\n",
    "\n",
    "- hoe hoger $x_n$, hoe hoger $\\hat{y}_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model)\n",
    "\n",
    "x = [-1, 4, 2, 0]\n",
    "softmax = model.SoftmaxLayer()\n",
    "y_hat = softmax.predict(x)\n",
    "print('Invoer  =', x)\n",
    "print('Uitvoer =', y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De implementatie die je hebt gekozen heeft waarschijnlijk een kleine tekortkoming: hij is nogal gevoelig voor numerieke *overflow* of *underflow*. De exponentiële functie kan al gauw leiden tot enorm grote of enorm kleine uitkomsten. Dit kan problemen veroorzaken.\n",
    "\n",
    "De code hieronder is nagenoeg identiek aan die hierboven, behalve dat alle $x_n$ worden verlaagd met een flinke offset door er -1000 bij op te tellen. Hierdoor worden de waarden $e^{x_n}$ zo klein dat deze worden afgerond naar nul omdat ze niet meer als een float kunnen worden gerepresenteerd. De som van de waarden is dan ook nul, hetgeen mogelijk leidt tot een `ZeroDivisionError`. Probeer dit maar eens uit hieronder.\n",
    "\n",
    "Het gaat ook mis als de waarden te groot zijn. Verander de offset eens naar `offset = +1000` en bekijk wat er dan gebeurt. Welke andere foutmelding krijg je nu? Kun je begrijpen wat er misgaat?\n",
    "\n",
    "Dit is te corrigeren door slim met de invoerwaarden om te gaan. De uitkomsten van de softmax-functie zijn eigenlijk alleen afhankelijk van de *verschillen* tussen de invoerwaarden. Ze veranderen als het goed is niet als alle invoerwaarden met dezelfde waarde worden verhoogd of verlaagd (zoals in de onderstaande code gedaan met de `offset` variabele). Je kan de invoerwaarden naar een bereik verschuiven dat niet leidt tot numerieke under-/overflow door van alle waarden het maximum van de hele lijst af te trekken, nog voordat je de exponentiële functie toepast. Dit zorgt ervoor dat in de lijst tenminste één waarde voorkomt gelijk aan $x_n=0$, en dat alle andere $x_n$ kleiner zijn. Overtuig jezelf ervan dat dit beide bovenstaande problemen oplost.\n",
    "\n",
    "Implementeer de beschreven stap en ga na dat daarmee de onderstaande code wel te draaien is met zowel `offset = -1000` als `offset = +1000`. Controleer ook dat de uitkomsten qua uitvoer identiek zijn aan die uit de vorige cel hierboven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model)\n",
    "\n",
    "offset = -1000\n",
    "x = [-1+offset, 4+offset, 2+offset, 0+offset]\n",
    "softmax = model.SoftmaxLayer()\n",
    "y_hat = softmax.predict(x)\n",
    "print('Invoer  =', x)\n",
    "print('Uitvoer =', y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De `loss()` functie is vervolgens relatief eenvoudig te implementeren. Deze zet de binnenkomende `x` om in een uitvoer (zoals gedaan door de `predict()` functie) en roept met deze uitvoerwaarde de loss-functie van de volgende laag in `self.next` aan. Die retourneert dan uiteindelijk een uitkomst voor de loss, die de `SoftmaxLayer()` terug kan leveren aan de gebruiker.\n",
    "\n",
    "Schrijf deze functie en test deze met de onderstaande code. Komt de waarde van de loss overeen met een eigen berekening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model)\n",
    "\n",
    "x = [-1, 4, 2, 0]\n",
    "y = [ 0, 1, 0, 0]\n",
    "softmax = model.SoftmaxLayer()\n",
    "softmax.append(model.LossLayer())\n",
    "y_hat = softmax.predict(x)\n",
    "l = softmax.loss(x, y)\n",
    "print('Invoer  =', x)\n",
    "print('Uitvoer =', y_hat)\n",
    "print('Labels  =', y)\n",
    "print('Loss    =', l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resteert alleen nog de `train()` functie. Deze dient back-propagation uit te voeren. De gradiënt van de loss naar de uitvoer wordt ontvangen van de volgende laag, omgerekend naar de gradiënt van de loss naar de invoer, en doorgegeven naar de aanroeper als return value. Net als de `LossLayer()` heeft de `SoftmaxLayer()` geen modelparameters om bij te werken; de learning rate wordt alleen als functieparameter gebruikt om compatibel te zijn met de voorafgaande neurale lagen.\n",
    "\n",
    "Maak gebruik van de formule\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial l}{\\partial x_{m}}=\\sum_{n}\\frac{\\partial l}{\\partial\\hat{y}_{n}}\\cdot\\hat{y}_{n}\\cdot\\left(\\delta_{mn}-\\hat{y}_{m}\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "om de gradiënt van de loss naar de invoer te berekenen en te retourneren (deze ene formule combineert twee formules uit de Syllabus). Je krijgt dan iets als dit:\n",
    "\n",
    "```\n",
    "def train(self, x, y, alpha=0)\n",
    "    prediction = self.predict(x)\n",
    "    output_gradient = self.next.train(prediction, y, alpha)\n",
    "    input_gradient = ...\n",
    "    return input_gradient\n",
    "```\n",
    "\n",
    "Tip: de Kronecker-delta $\\delta_{mn}$ kun je in Python uitdrukken als `(m == n)`; dit levert een boolean op, maar deze wordt bij berekeningen automatisch gecast naar een waarde gelijk aan 0 (voor `False`) of 1 (voor `True`).\n",
    "\n",
    "Als je deze code werkend hebt gekregen kun je als het goed is hieronder de eerdere fractale dataset fitten met een model inclusief een softmax laag. Is nu de uitvoer van je model wel te interpreteren als een kans? Dat wil zeggen, zijn de voorspellingen voor alle klassen altijd groter dan nul, en tellen ze op tot één? Komen de fits evengoed redelijk overeen met de verdeling van de datapunten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model)\n",
    "\n",
    "hidden1 = model.FullLayer(2, 8, act_func=model.tanh_act_func)\n",
    "hidden2 = model.FullLayer(8, 3, act_func=model.identity_act_func)\n",
    "output = model.SoftmaxLayer()\n",
    "loss = model.LossLayer(loss_func=model.quadratic_loss_func)\n",
    "my_network = hidden1 + hidden2 + output + loss\n",
    "my_network.fit(xs, ys, alpha=0.1)\n",
    "data.scatter(xs, ys, model=my_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy\n",
    "\n",
    "*Het vervolg van het werkcollege gaat in op het implementeren van de cross-entropy loss-functie. Instructies zullen nog volgen...*\n",
    "\n",
    "*(Download de meest recente versie van dit notebook van BlackBoard.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model)\n",
    "\n",
    "data.graph([model.quadratic_loss_func, model.crossentropy_loss_func], 0.0, xmin=0.0, xmax=1.0)\n",
    "data.graph([model.quadratic_loss_func, model.crossentropy_loss_func], 1.0, xmin=0.0, xmax=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model)\n",
    "\n",
    "hidden = model.FullLayer(2, 8, act_func=model.tanh_act_func)\n",
    "output = model.FullLayer(8, 3, act_func=model.identity_act_func)\n",
    "softmax = model.SoftmaxLayer()\n",
    "loss = model.LossLayer(loss_func=model.crossentropy_loss_func)\n",
    "my_network = hidden + output + softmax + loss\n",
    "my_network.fit(xs, ys, alpha=0.1)\n",
    "data.scatter(xs, ys, model=my_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
